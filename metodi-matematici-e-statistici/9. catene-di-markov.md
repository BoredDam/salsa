# **Processi stocastici e Catene di Markov**

## **Processo stocastico**

Un processo stocastico è una famiglia di variabili aleatorie. È una successione di variabili aleatorie, cui indice può rappresentare, ad esempio, lo scorrere del tempo.

$$
X(t)_{t \in \mathbb{N}}
$$

## **Catene di Markov**

Un processo stocastico è detto catena di Markov, se

$$
P(X_{t+1} = j | X_{t} = i, X_{t-1} = i_{t-1}, \dots, X_{t=0} = i_{0}) = P(X_{t+1} = j | X_{t} = i_{t})
$$

E quindi, se gode della proprietà di **mancanza di memoria**. 

Assumeremo inoltre che le catene di Markov siano tutte omogenee, ossia

$$
P_{ij}(t) = P_{ij}
$$

### **Matrice di transizione**

Le matrici di transizione permettono di rappresentare le probabilità di passare da uno stato $i \to j, \ \forall i,j \in E^2$.

$$
P = \begin{pmatrix}
P_{1,1} & P_{1,2} & \dots & P_{1,n}\\
\vdots & \vdots & \ddots & \vdots\\
P_{n,1} & P_{n,2} & \dots & P_{n,n}
\end{pmatrix}
$$

- $P_{i,j} \geq 0 \ \forall i,j$, *la probabilità di passare da uno stato $i$ a uno stato $j$ è sempre non negativa*.
- $\sum_{j \in E}P_{ij} = 1$,  *la probabilità di passare ad uno stato qualsiasi è sempre 1*.

Un caso particolare è quello dei **processi bistocastici**, in cui è vero anche che $\sum_{i \in E}P_{ij} = 1$.

### **Teorema di Chapman Kolmogorov**

Per conoscere la probabilità di essere in qualsiasi stato dopo $m$ passi, calcoliamo la potenza $m$-esima della matrice.

$$
P^{(m)} = \underbrace{P\times P \times\dots\times P}_{m \text{ volte}} 
$$

## **Stati di una catena di Markov e alcune definizioni**

**Stati comunicanti.** $i, j$ si dicono comunicanti se $\exist m \in \mathbb{N}$ tale che la probabilità di andare da $i, j$ in $m$ passi è $>0$. Si dirà
$$
i \longrightarrow j, \qquad \text{altrimenti} \qquad i \cancel{\longrightarrow} j
$$

**Catena irriducibile.** Se tutti gli stati sono comunicanti, la catena di Markov si dice **irriducibile**.

**Classi chiuse.** Se gli stati di $C \subset E$ non comunicano con gli stati di $E \backslash C$, la classe $C$ si dice **classe chiusa**.

**Classi irriducibili.** $C$ è irriducibile se tutti i suoi stati comunicano.

**Stato assorbente.** Lo stato di una classe assorbente con un solo stato, è detto **stato assorbente**.

>*Le catene di Markov sono da vedersi come sistemi in evoluzione. Fermarsi su uno stato assorbente, significa raggiungere la fine dell'evoluzione.*

**Stato ricorrente e transitorio.** Uno stato si dice **ricorrente** se ho probabilità 1 di tornarci ad un certo tempo $t$, altrimenti si dice **transitorio**.


## **Leggi e leggi stazionarie**


La **legge** mi dice qual è la probabilità, ad ogni istante di tempo $t$, di trovarmi in un dato stato.

### **Legge al passo 0**

$X_0$ è lo stato all'istante iniziale e $v$ è la sua legge

$$
v = (v_1,v_2,\dots,v_n) \qquad \text{ con } n = |E|
$$

tale che

- $v_k = P(X_0 = k)$
- $\displaystyle \sum_{k \in E} v_k = 1$.

### **Legge al passo $t$**

$w$ si dice legge al passo $n$, ovvero $w = (w_1, \dots,w_n)$ se $w_k = P(X_t = k)$. 

È facile vedere che, con $P^{(n)}$ matrice e $v$ vettore

$$
w = v P^{(t)}
$$

ovvero che, nota la matrice di transizione e la distribuzione (legge) dello stato iniziale $v$, si può ottenere una distribuzione $w = (w_1, \dots,w_n)$ che esprime la probabilità di giungere all'istante $t$ in ciascuno dei possibili stati.

### **Legge stazionaria**
---

Una distribuzione $v$ si dice **stazionaria** se 

$$
v = vP
$$

questo esprime una stazionarietà della catena di Markov. Vale quindi che $w_{(t=x)} = w_{(t=x+k)} \forall k \in \mathbb{N}$. Rappresenta una condizione del sistema, in cui le probabilità si stabilizzano definitivamente.

### **Teorema di Markov-Kakutani**

Se $E$ è finito, allora esiste almeno una distribuzione stazionaria. È un teorema molto importante, in quanto spesso l'obiettivo di chi studia catene di Markov, è proprio quello di trovare una legge stazionaria.


### **Teorema di Markov**

Se $P$ è regolare (ossia non contiene 0) ed $E$ è finito, allora esiste esclusivamente una sola distribuzione stazionaria, a cui si convergerà.


> ***Osservazione 1.*** *Otteniamo quindi un modo numerico per calcolre le distribuzioni stazionarie.*

> ***Osservazione 2.*** *Non importa da dove si parte, alla fine si converge sempre alla stessa distribuzione stazionaria.*

![](https://preview.redd.it/what-does-this-mean-i-see-it-everywhere-v0-2tk6silssjwf1.png?width=320&crop=smart&auto=webp&s=093299164326bcc46ffe47071876fc7e877f339f)




## Algoritmo di Metropolis, *Simulated annealing* - non in esame

È un algoritmo per trovare i minimi di una funzione $H(i)$. *Quali sono i parametri che minimizzano l'energia?* *Qual è la scelta migliore nel commesso viaggiatore?*

1. Mi creo una matrice di transizione ausiliaria

$$
\Pi_i = \frac{e^{-H(\varepsilon)/\varepsilon}}{Z_{H(\varepsilon)}} 
$$

2. Mi costruisco ...

3. Eseguo la mia simulazione partendo da uno stato di partenza.

4. Applico iterativamente la procedura: passo al prossimo stato più probabile?

Devo riaggiustare questa parte. Anyway, questo algoritmo (detto algoritmo di Metropolis) funziona perché la matrice di transizione costruita è *regolare*, e quindi la catena ha distribuzione stazionaria.