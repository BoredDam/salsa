# Alcune distribuzioni discrete

## Discreta uniforme
Descrive fenomeni stocastici con tanti esiti, finiti o numerabili, tutti con la stessa probabilità. 

### Legge

$$
p(x) = \begin{cases}
\displaystyle\frac{1}{n} \quad \text{ se }  x \in X(E)\\
0 \qquad \text{altrimenti}
\end{cases}
$$

### Valore atteso

$$
E[X] = \frac{n+1}{2}
$$

### Varianza

$$
Var(X) = \frac{n^2-1}{12}
$$

### Funzione di ripartizione

$$
F_X(t) = P(X \leq t) = \sum_{X \leq t} p(x) = \begin{cases} 
1/n \quad t = 1\\  
2/n \quad t = 2\\
\vdots\\
1 \quad t = n
\end{cases}
$$

### Plot
![Distribuzione uniforme discreta](https://upload.wikimedia.org/wikipedia/commons/thumb/1/13/DUniform_distribution.svg/960px-DUniform_distribution.svg.png)


## Distribuzione binomiale
Descrive un fenomeno con due esiti e di cui si effettuano $n$ prove indipendenti.

- $n$ prove
- $p$ probabilità di successo ($1-p = q$ insuccesso)
- $k$ numero di successi

Una variabile aleatoria $X$ segue una **legge binomiale** di parametri $n$ e $p$ se ha densità discreta

$$
p(x) = \begin{cases}
\displaystyle\binom{n}{x} p^x(1-p)^{n-x} \quad \text{se } x=0,1,\dots,n\\
0 \qquad\qquad\qquad\qquad \text{altrimenti}
\end{cases}
$$

E si scriverà $X \sim B(n,p)$.

### Valore atteso

$$
E[X] = np
$$

### Varianza

$$
Var(X) = np(1-p)
$$

### Plot
![Distribuzione binomiale](https://upload.wikimedia.org/wikipedia/commons/thumb/7/75/Binomial_distribution_pmf.svg/2560px-Binomial_distribution_pmf.svg.png)


## Distribuzione di Poisson
Presa la binomiale, se di questa fai $n\to \infty$ e $p \to 0$, e $np = \lambda$. Si scrive come $X \sim Pois(\lambda), \ \lambda = np$

$$
p(x) = \begin{cases}
e^{-\lambda} \displaystyle \frac{\lambda^x}{x!} \qquad\text{se } x=0,1,\dots, +\infty\\
0 \qquad\qquad \text{altrimenti}
\end{cases}
$$

### Valore atteso

$$
E[X] = \lambda
$$

### Varianza

$$
Var(X) = \lambda
$$

### Plot
![Distribuzione di Poisson](https://upload.wikimedia.org/wikipedia/commons/thumb/1/16/Poisson_pmf.svg/960px-Poisson_pmf.svg.png)


## Distribuzione Multinomiale
Descrive uno schema successo insuccesso con più di due esiti. Ogni esito ha probabilità $q_i$.

$$
Y \sim B(n, q_1, q_2, \dots , q_k ).
$$


## Distribuzione geometrica

$X \sim Geom(p)$. Valuta il tempo di primo successo, ossia il numero di tentativi necessario per riscontrare il primo successo in un processo di tipo binomiale (successo/non successo).

$$
p(x) = \begin{cases}
p(1-p)^{x-1} \qquad \text{per } x =1, 2, \dots, \infty\\
0 \qquad\qquad\qquad \text{ altrimenti}
\end{cases}
$$

Gode della mancanza di memoria: il tempo di primo successo non è influenzato dagli esiti precedenti.

### Valore atteso
$$
E[X] = \frac{1}{p}
$$

### Varianza
$$
Var(X) = \frac{1-p}{p^2}
$$

### Plot
![Distribuzione Geometrica](https://upload.wikimedia.org/wikipedia/commons/thumb/5/5e/Geometric_distribution_CDF.svg/2560px-Geometric_distribution_CDF.svg.png)

## Distribuzione ipergeometrica

Descrive **estrazioni senza rimpiazzo** (la tombola, dei sacchi di palline colorate in quantità finite). Le estrazioni, chiaramente, **non sono indipendenti**. Se estraggo un elemento, la probabilità varia.
	
Abbiamo $k$ palline con una proprietà, ed $n-k$ palline senza quella proprietà. Le supponiamo tutte equiprobabili. $X$ è la variabile aleatorie che conta quante volte ho estratto la pallina con la proprietà scelta. La densità, dopo $m$ estrazioni
	
$$
p(x) = \begin{cases} \frac{\displaystyle\binom{k}{x}\binom{n-k}{m-x}}{\displaystyle\binom{n}{m}} \qquad \text{se } x = 0, 1, \dots, n\\
0 \qquad\qquad\qquad\qquad \text{altrimenti}

\end{cases}
$$

### Speranza matematica
	
$$
E[X] = \frac{mk}{n}
$$
	
### Varianza

$$
Var(X) = \frac{mk(n-k)}{n^2} \frac{m-k}{m-1}
$$

### Plot
![Distribuzione Ipergeometrica](https://www.mql5.com/it/docs/img/demohypergeometricdistribution.png)