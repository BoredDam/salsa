# Entropia

[...]


## Entropia e disordine

Non esiste alcuna grandezza fisica capace di quantificare il disordine di un sistema, nemmeno l'entropia.

Non essendo possibile quantificare il disordine, l'entropia viene erroneamente asssociata al disordine.

E più corretto legare l'entropia alla probabilità termodinamica $W$. È una grandezza fisica che conta quanti microstati corrispondono ad uno stato macroscopico. Più grande è questa probabilità, più grande è il numero di combinazioni di stati microscopici che creano un determinato macrostato.

### Esempio: due dadi

Lanciando due dadi, possiamo ottenere da

$$
\{2\} = |1\rangle|1\rangle
$$

$$
\{7\} = |1\rangle|6\rangle + |2\rangle|5\rangle + |3\rangle|4\rangle + |4\rangle|3\rangle + |5\rangle|2\rangle + |6\rangle|1\rangle
$$

## Entropia e probabilità termodinamica

Boltzmann lega questi due valori nella seguente legge:

$$
S = k_b \log W
$$

### Secondo principio della Termodinamica

**Le trasformazioni spontanee sono quelle che comportano un aumento della probabilità termodinamica**. (La probabilità termodinamica aumenta con stati macroscopici replicabili con più stati microscopici, ovvero negli stati più probabili).

### Esempio di due molecole in un recipiente

[...]

È evidente che lo stato con $N_S = N_D = 1$ è più probabile.

### Esempio con $n$ molecole in un recipiente

Concludiamo che esiste una distribuzione di probabilità, in cui il picco (molto molto molto alto) sulla configurazione più probabile, cresce col numero di molecole.

### Definitivamente

$$
\text{Aumento di } W \Leftrightarrow \text{Aumento dell'Entropia}
$$

### Analogia con l'Entropia di Shannon

Se l'entropia cresce, diminuiscono le informazioni relative ai microstati, esattamente come con l'entropia di Shannon.